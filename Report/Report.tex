\documentclass[12pt,a4paper]{article}
\usepackage[a4paper, portrait, margin = 1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfig}
\graphicspath{ {./images/} }
\usepackage{biblatex}
\addbibresource{References.bib}

\newcommand{\x}{\boldsymbol{x}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\np}{\vskip 5mm}

\hbadness=3000

\title{Sequential Monte Carlo Methods}
\author{Andrea Becsek, Mauro Camara, Doug Corbin}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

To be completed once we have a better understanding of SMC methods.

\section{Sampling Methods}

Suppose that, for some random variable $\X$ with probability density function $p$, we wish to compute the expectation
\begin{equation}
\label{expectation}
\mathbb{E}[f(\X)] = \int f(\x) p(\x) \text{d}\x
\end{equation}
for some function $f$. In many cases, $f$ and $p$ are such that \eqref{expectation} cannot be computed analytically, therefore we must look for suitable approximations. Consider a random sample $\x_1, \ldots, \x_m$ drawn from $p$, and compute the sample mean
\begin{equation}
\label{sample mean}
\bar{f}^*_m \coloneqq \frac{1}{m} \sum_{i = 1}^m f(\x_i) \text{.}
\end{equation}
From the weak law of large numbers, it follows that
\begin{equation}
\lim_{m \rightarrow \infty} \mathbb{P}(|\bar{f}^*_m - \mathbb{E}[f]| > \epsilon) = 0
\end{equation}
for any $\epsilon > 0 $. This result forms the basis of sampling/monte carlo methods, which approximate integrals of the form \eqref{expectation} by a discrete sum of samples drawn from $p$. However, in practice we are unable to sample directly from $p$, thus we require sampling methods. In this section we cover sampling methods which are most relevant to Sequential Monte Carlo (SMC) methods, however a general overview of sampling methods can be found in \cite[Chapter 11]{bishop:prml}.

\subsection{Importance Sampling}

The key idea of importance sampling is to approximately sample from $p$ by first drawing samples $\x_1, \ldots, \x_m$ from some \textit{trial distribution} $q()$, then computing
\begin{equation}
\bar{f}_m \coloneqq \sum_{i = 1}^m w_i f(\x_i)
\end{equation}
where $\boldsymbol{w} = (w_1, \ldots, w_m)$ are weights which correct for the bias introduced by sampling from $q$ instead of $p$. We start by deriving how we construct $\boldsymbol{w}$, then we proceed with an explanation of the trial distirbution $q()$ and how it is chosen.
\np
Assuming $p(\x)$, $q(\x)$ are known up to a constant (i.e. $p(\x) = \tilde{p}(\x) / C_p $ and \\ $q(\x) = \tilde{q}(\x) / C_q$), we can construct the following weights:
\begin{equation}
\begin{split}
\mathbb{E}[f] &= \int f(\x) p(\x) \text{d}x \\
&= \frac{C_q}{C_p} \int f(\x) \frac{\tilde{p}(\x)}{\tilde{q}(\x)} q(\x) \text{d}\x \\
&\approx \frac{C_q}{C_p} \sum_{i = 1}^m \frac{\tilde{p}(\x_i)}{\tilde{q}(\x_i)} f(\x_i) \\
&= \frac{C_q}{C_p} \sum_{i = 1}^m w^*_i f(\x_i)
\end{split}
\end{equation}
Where we have defined $w^*_i =\tilde{p}(\x_i) / \tilde{q}(\x_i)$. The ratio of normalization constants can be approximated as
\begin{equation}
\begin{split}
\frac{C_p}{C_q} &= \frac{1}{C_q} \int \tilde{p}(\x) \text{d}\x = \int \frac{\tilde{p}(\x)}{\tilde{q}(\x)} q(\x) \text{d}\x \\
&\approx \sum_{i = 1}^m w_i^*.
\end{split}
\end{equation}
Hence the final expression for $w_i$ is written as
\begin{equation}
w_i = \frac{w^*_i}{\sum_{i = 1}^m w^*_i} = \frac{\tilde{p}(\x_i) / \tilde{q}(\x_i)}{\sum_{i = 1}^m \tilde{p}(\x_i) / \tilde{q}(\x_i)}.
\end{equation}


\printbibliography

\end{document}